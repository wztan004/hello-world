{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WZ: Would suggest to use the source code on github as the correct version of code as the tutorial site can be a little wrong\n",
    "###### Topics: One-hot encoding, decision tree, random forest\n",
    "\n",
    "###### Tutorial: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789616729/6/ch06lvl1sec50/ensembling-decision-trees-random-forest\n",
    "###### Source code: https://github.com/haydenliu/Python-Machine-Learning-By-Example-Second-Edition/blob/master/Chapter06/avazu_ctr_tf.py\n",
    "###### Data source: https://www.kaggle.com/c/avazu-ctr-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
      "       'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
      "       'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
      "       'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "n_rows = 30000\n",
    "df = pd.read_csv(\"train.csv\", nrows=n_rows)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The target variable is the click column.\n",
    "##### Several columns do not contain much useful information and are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 19)\n"
     ]
    }
   ],
   "source": [
    "# wz: edited the official code. Changed from .values to .to_numpy()\n",
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).to_numpy()\n",
    "Y = df['click'].to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample has 19 predictive attributes.\n",
    "\n",
    "Next, we need to split the data into training and testing sets. Normally, we do so by randomly picking samples. However, in our case, samples are in chronological order as indicated in the hour field. Obviously, we cannot use future samples to predict the past ones. Hence, we take the first 90% as training samples and the rest as testing samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(n_rows * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to training set and test set\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, decision tree models can take in categorical features. However, because the tree-based algorithms in scikit-learn (the current version is 0.20.0 as of the end of 2018) only allow numerical input, we need to transform categorical features into numerical ones. But note that in general we do not need to do so; for example, the decision tree classifier we developed from scratch earlier can directly take in categorical features.\n",
    "\n",
    "We now transform string-based categorical features into one-hot encoded vectors using the OneHotEncoder module from scikit-learn. It basically converts a categorical feature with k possible values into k binary features. For example, the site category feature with three possible values, news, education, and sports, will be encoded into three binary features, such as is_news, is_education, and is_sports, whose values are either 1 or 0.\n",
    "\n",
    "Initialize a OneHotEncoder object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3929 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding process\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_train_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 19)\n",
      "(27000, 3929)\n"
     ]
    }
   ],
   "source": [
    "# shape before one hot encoding\n",
    "print(X_train.shape)\n",
    "print(X_train_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 71)\t1.0\n",
      "  (0, 1002)\t1.0\n",
      "  (0, 1024)\t1.0\n",
      "  (0, 1460)\t1.0\n",
      "  (0, 1508)\t1.0\n",
      "  (0, 1529)\t1.0\n",
      "  (0, 2016)\t1.0\n",
      "  (0, 3257)\t1.0\n",
      "  (0, 3261)\t1.0\n",
      "  (0, 3361)\t1.0\n",
      "  (0, 3605)\t1.0\n",
      "  (0, 3609)\t1.0\n",
      "  (0, 3644)\t1.0\n",
      "  (0, 3735)\t1.0\n",
      "  (0, 3740)\t1.0\n",
      "  (0, 3775)\t1.0\n",
      "  (0, 3915)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_enc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each converted sample is a sparse vector.\n",
    "\n",
    "Transform the testing set using the trained one-hot encoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_enc = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we specify the handle_unknown='ignore' parameter in the one-hot encoder earlier. This is to prevent errors due to any unseen categorical values. Use the previous site category example, if there is a sample with the value movie, three converted binary features (is_news, is_education, and is_sports) all become 0s. If we do not specify ignore, an error will be raised.  \n",
    "\n",
    "Next, we train a decision tree model using grid search, which we learned about in Chapter 5, Classifying Newsgroup Topics with a Support Vector Machine. For demonstration purposes, we only tweak the max_depth hyperparameter. Other hyperparameters, such as min_samples_split and class_weight, are also highly recommended. The classification metric should be AUC of ROC, as it is an imbalanced binary case (only 51,211 out of 300,000 training samples are clicks, that, is a 17% positive click-through rate):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pick three options for the maximal depth, 3, 10, and unbounded. Initialize a decision tree model with Gini Impurity as the metric and 30 as the minimum number of samples required to split further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'max_depth': [3, 10, None]}\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini',\n",
    "                                       min_samples_split=30)\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As for grid search, we use three-fold (as there are enough training samples) cross-validation and select the best performing hyperparameter measured by AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(decision_tree, parameters,\n",
    "                           n_jobs=-1, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note n_jobs=-1 means that we use all available CPU processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_enc, Y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the model with the optimal parameter to predict future test cases, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set is: 0.679\n"
     ]
    }
   ],
   "source": [
    "decision_tree_best = grid_search.best_estimator_\n",
    "pos_prob = decision_tree_best.predict_proba(X_test_enc)[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('The ROC AUC on testing set is: {0:.3f}'.format(roc_auc_score(Y_test, pos_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The AUC we can achieve with the optimal decision tree model is 0.72. It does not seem very high, but click-through involves many intricate human factors, which is why predicting it is not an easy problem. Although we can further optimize its hyperparameters, an AUC of 0.72 is pretty good, actually. Randomly selecting 17% of the samples to be click will generate an AUC of 0.496:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49076304543945526"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_prob = np.zeros(len(Y_test))\n",
    "click_index = np.random.choice(len(Y_test),\n",
    "                               int(len(Y_test) * 51211.0/300000), replace=False)\n",
    "pos_prob[click_index] = 1\n",
    "roc_auc_score(Y_test, pos_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking back, we can see that a decision tree is a sequence of greedy searches for the best splitting point at each step, based on the training dataset. However, this tends to cause overfitting as it is likely that the optimal points only work well for the training samples. Fortunately, ensembling is the technique to correct this, and random forest is an ensemble tree model that usually outperforms a simple decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The ensemble technique bagging (which stands for bootstrapaggregating) can effectively overcome overfitting. To recap, different sets of training samples are randomly drawn with replacements from the original training data; each resulting set is used to fit an individual classification model. The results of these separately trained models are then combined together through a majority vote to make the final decision.\n",
    "\n",
    "##### Tree bagging, described in the preceding section, reduces the high variance that a decision tree model suffers from and hence, in general, performs better than a single tree. However, in some cases, where one or more features are strong indicators, individual trees are constructed largely based on these features and as a result become highly correlated. Aggregating multiple correlated trees will not make much difference. To force each tree to be uncorrelated, random forest only consider a random subset of the features when searching for the best splitting point at each node. Individual trees are now trained based on different sequential sets of features, which guarantees more diversity and better performance. Random forest is a variant tree bagging model with additional feature-based bagging.\n",
    "\n",
    "##### To employ random forest in our click-through prediction project, we use the package from scikit-learn. Similar to the way we implemented the decision tree in the preceding section, we only tweak the max_depth parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100,\n",
    "                                       criterion='gini', min_samples_split=30, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Besides max_depth, min_samples_split, and class_weight, which are important hyperparameters related to a single decision tree, hyperparameters that are related to a random forest (a set of trees) such as n_estimators are also highly recommended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None}\n",
      "0.7185197803281614\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(random_forest, parameters,\n",
    "                           n_jobs=-1, cv=3, scoring='roc_auc')\n",
    "grid_search.fit(X_train_enc, Y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the model with the optimal parameter None for max_depth (nodes are expanded until another stopping criterion is met) to predict future unseen cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC on testing set is: 0.707\n"
     ]
    }
   ],
   "source": [
    "random_forest_best = grid_search.best_estimator_\n",
    "pos_prob = random_forest_best.predict_proba(X_test_enc)[:, 1]\n",
    "print('The ROC AUC on testing set is: {0:.3f}'.format(roc_auc_score(Y_test, pos_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth: This is the deepest individual tree. It tends to overfit if it is too deep, or to underfit if it is too shallow.\n",
    "##### min_samples_split: This hyperparameter represents the minimum number of samples required for further splitting at a node. Too small a value tends to cause overfitting, while too large a value is likely to introduce underfitting. 10, 30, and 50 might be good options to start with.\n",
    "##### max_features: This parameter represents the number of features to consider for each best splitting point search. Typically, for an m-dimensional dataset, (rounded) is a recommended value for max_features. This can be specified as max_features=\"sqrt\" in scikit-learn. Other options include log2, 20% of the original features to 50%.\n",
    "##### n_estimators: This parameter represents the number of trees considered for majority voting. Generally speaking, the more trees, the better the performance, but more computation time. It is usually set as 100, 200, 500, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> n_iter = 20\n",
    "# >>> n_classes = 2\n",
    "# >>> n_features = int(X_train_enc.toarray().shape[1])\n",
    "# >>> n_trees = 10\n",
    "# >>> max_nodes = 30000\n",
    "\n",
    "# >>> x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "# >>> y = tf.placeholder(tf.int64, shape=[None])\n",
    "# >>> hparams = tensor_forest.ForestHParams(num_classes=n_classes,\n",
    "#  num_features=n_features, num_trees=n_trees,\n",
    "#  max_nodes=max_nodes, split_after_samples=30).fill()\n",
    "# >>> forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "\n",
    "# >>> train_op = forest_graph.training_graph(x, y)\n",
    "# >>> loss_op = forest_graph.training_loss(x, y)\n",
    "# >>> infer_op, _, _ = forest_graph.inference_graph(x)\n",
    "# >>> auc = tf.metrics.auc(tf.cast(y, tf.int64), infer_op[:, 1])[1]\n",
    "\n",
    "# >>> init_vars = tf.group(tf.global_variables_initializer(),\n",
    "#           tf.local_variables_initializer(),\n",
    "#        resources.initialize_resources(resources.shared_resources()))\n",
    "# >>> sess = tf.Session()\n",
    "# >>> sess.run(init_vars)\n",
    "\n",
    "# >>> batch_size = 1000\n",
    "# >>> import numpy as np\n",
    "# >>> indices = list(range(n_train))\n",
    "# >>> def gen_batch(indices):\n",
    "# ...     np.random.shuffle(indices)\n",
    "# ...     for batch_i in range(int(n_train / batch_size)):\n",
    "# ...         batch_index = indices[batch_i*batch_size:\n",
    "#                                 (batch_i+1)*batch_size]\n",
    "# ...         yield X_train_enc[batch_index], Y_train[batch_index]\n",
    "\n",
    "# >>> for i in range(1, n_iter + 1):\n",
    "# ...     for X_batch, Y_batch in gen_batch(indices):\n",
    "# ...         _, l = sess.run([train_op, loss_op], feed_dict=\n",
    "#                            {x: X_batch.toarray(), y: Y_batch})\n",
    "# ...     acc_train = sess.run(auc, feed_dict=\n",
    "#                            {x: X_train_enc.toarray(), y: Y_train})\n",
    "# ...     print('Iteration %i, AUC of ROC on training set: %f' %\n",
    "#                                                   (i, acc_train))\n",
    "# ...     acc_test = sess.run(auc, feed_dict=\n",
    "#                            {x: X_test_enc.toarray(), y: Y_test})\n",
    "# ...     print(\"AUC of ROC on testing set:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
